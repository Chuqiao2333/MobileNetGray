{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MobileNet.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chuqiao2333/MobileNetGray/blob/master/MobileNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLD92gPEnmEJ",
        "colab_type": "text"
      },
      "source": [
        "# MobileNets V2 on GrayScale Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6U01QHdNVV57",
        "colab_type": "code",
        "outputId": "63d76bb2-63d5-4d34-9098-d9400631463e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jan  5 01:41:46 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHvqKzh1nsCc",
        "colab_type": "code",
        "outputId": "14088d0c-ea45-4b69-9676-91d32386b80c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYZ-o2mInslJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "import torch.nn as nn\n",
        "import h5py\n",
        "import time\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfUo4HkSqLJd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 100\n",
        "learning_rate = 0.0005\n",
        "batch_size = 128\n",
        "dropout_prob = 0.2\n",
        "dropout_prob_1d = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bh8sFXVqmpx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform_train = transforms.Compose([\n",
        "    #transforms.Resize(224),\n",
        "    transforms.Grayscale(1),\n",
        "    #transforms.RandomCrop(64, padding=4),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    #transforms.RandomRotation(45),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    #transforms.Resize(224),\n",
        "    transforms.Grayscale(1),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuxRLJ-m-Qz7",
        "colab_type": "code",
        "outputId": "bab90492-fc67-4f35-b827-3274932a90a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#train_dir = '/content/drive/My Drive/hw/tiny-imagenet-200 3/train'\n",
        "train_dataset = torchvision.datasets.CIFAR100(root='/content/drive/My Drive/hw',train=True,download=True, transform=transform_train)\n",
        "print(train_dataset.class_to_idx)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "{'apple': 0, 'aquarium_fish': 1, 'baby': 2, 'bear': 3, 'beaver': 4, 'bed': 5, 'bee': 6, 'beetle': 7, 'bicycle': 8, 'bottle': 9, 'bowl': 10, 'boy': 11, 'bridge': 12, 'bus': 13, 'butterfly': 14, 'camel': 15, 'can': 16, 'castle': 17, 'caterpillar': 18, 'cattle': 19, 'chair': 20, 'chimpanzee': 21, 'clock': 22, 'cloud': 23, 'cockroach': 24, 'couch': 25, 'crab': 26, 'crocodile': 27, 'cup': 28, 'dinosaur': 29, 'dolphin': 30, 'elephant': 31, 'flatfish': 32, 'forest': 33, 'fox': 34, 'girl': 35, 'hamster': 36, 'house': 37, 'kangaroo': 38, 'keyboard': 39, 'lamp': 40, 'lawn_mower': 41, 'leopard': 42, 'lion': 43, 'lizard': 44, 'lobster': 45, 'man': 46, 'maple_tree': 47, 'motorcycle': 48, 'mountain': 49, 'mouse': 50, 'mushroom': 51, 'oak_tree': 52, 'orange': 53, 'orchid': 54, 'otter': 55, 'palm_tree': 56, 'pear': 57, 'pickup_truck': 58, 'pine_tree': 59, 'plain': 60, 'plate': 61, 'poppy': 62, 'porcupine': 63, 'possum': 64, 'rabbit': 65, 'raccoon': 66, 'ray': 67, 'road': 68, 'rocket': 69, 'rose': 70, 'sea': 71, 'seal': 72, 'shark': 73, 'shrew': 74, 'skunk': 75, 'skyscraper': 76, 'snail': 77, 'snake': 78, 'spider': 79, 'squirrel': 80, 'streetcar': 81, 'sunflower': 82, 'sweet_pepper': 83, 'table': 84, 'tank': 85, 'telephone': 86, 'television': 87, 'tiger': 88, 'tractor': 89, 'train': 90, 'trout': 91, 'tulip': 92, 'turtle': 93, 'wardrobe': 94, 'whale': 95, 'willow_tree': 96, 'wolf': 97, 'woman': 98, 'worm': 99}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnVWCRkEqxZm",
        "colab_type": "code",
        "outputId": "b0e6ff6b-8a93-4ef6-8d49-c0cf1b50f75e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "val_dataset = torchvision.datasets.CIFAR100(root='/content/drive/My Drive/hw',train=False,download=True, transform=transform_val)\n",
        "print(val_dataset.class_to_idx)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "{'apple': 0, 'aquarium_fish': 1, 'baby': 2, 'bear': 3, 'beaver': 4, 'bed': 5, 'bee': 6, 'beetle': 7, 'bicycle': 8, 'bottle': 9, 'bowl': 10, 'boy': 11, 'bridge': 12, 'bus': 13, 'butterfly': 14, 'camel': 15, 'can': 16, 'castle': 17, 'caterpillar': 18, 'cattle': 19, 'chair': 20, 'chimpanzee': 21, 'clock': 22, 'cloud': 23, 'cockroach': 24, 'couch': 25, 'crab': 26, 'crocodile': 27, 'cup': 28, 'dinosaur': 29, 'dolphin': 30, 'elephant': 31, 'flatfish': 32, 'forest': 33, 'fox': 34, 'girl': 35, 'hamster': 36, 'house': 37, 'kangaroo': 38, 'keyboard': 39, 'lamp': 40, 'lawn_mower': 41, 'leopard': 42, 'lion': 43, 'lizard': 44, 'lobster': 45, 'man': 46, 'maple_tree': 47, 'motorcycle': 48, 'mountain': 49, 'mouse': 50, 'mushroom': 51, 'oak_tree': 52, 'orange': 53, 'orchid': 54, 'otter': 55, 'palm_tree': 56, 'pear': 57, 'pickup_truck': 58, 'pine_tree': 59, 'plain': 60, 'plate': 61, 'poppy': 62, 'porcupine': 63, 'possum': 64, 'rabbit': 65, 'raccoon': 66, 'ray': 67, 'road': 68, 'rocket': 69, 'rose': 70, 'sea': 71, 'seal': 72, 'shark': 73, 'shrew': 74, 'skunk': 75, 'skyscraper': 76, 'snail': 77, 'snake': 78, 'spider': 79, 'squirrel': 80, 'streetcar': 81, 'sunflower': 82, 'sweet_pepper': 83, 'table': 84, 'tank': 85, 'telephone': 86, 'television': 87, 'tiger': 88, 'tractor': 89, 'train': 90, 'trout': 91, 'tulip': 92, 'turtle': 93, 'wardrobe': 94, 'whale': 95, 'willow_tree': 96, 'wolf': 97, 'woman': 98, 'worm': 99}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9Wtrr6t0Syi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "upsample = nn.Upsample(scale_factor=8, mode='bilinear') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDcIxTPo-HbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Preprocess(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Preprocess, self).__init__()\n",
        "\n",
        "        self.upsample = nn.Upsample(scale_factor=7, mode = 'bicubic')\n",
        "        self.conv1    = nn.Conv2d(1,3,3,stride = 1, padding=1)\n",
        "        self.conv2    = nn.Conv2d(3,10,3, stride=1, padding = 1)\n",
        "        self.conv3    = nn.Conv2d(10,3,3, stride=1, padding = 1)\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        \n",
        "        x = self.upsample(x)\n",
        "        input_X = self.conv1(x)\n",
        "        x = self.conv2(input_X)\n",
        "        x = self.conv3(x)\n",
        "        x = x + input_X\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tklKhrBE_L-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "\n",
        "    model = torch.hub.load('pytorch/vision:v0.4.2', 'mobilenet_v2', pretrained=True)\n",
        "    model.classifier = nn.Linear(1280, 100)\n",
        "\n",
        "    return model\n",
        "class MobileNet_Gray(nn.Module):\n",
        "\n",
        "    def __init__(self, Preprocess):\n",
        "\n",
        "        super(MobileNet_Gray, self).__init__()\n",
        "\n",
        "        self.one2three = Preprocess()\n",
        "        self.model     = create_model()\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        x = self.one2three(x)\n",
        "        x = self.model(x)\n",
        "\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiCXT33L5gI6",
        "colab_type": "code",
        "outputId": "34f73b36-8fa4-4421-9598-5675b32fb62b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = MobileNet_Gray(Preprocess)\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "temp_model = Preprocess()\n",
        "temp_model.cuda()\n",
        "from torchsummary import summary\n",
        "summary(model, (1,32,32))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.4.2\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2494: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "          Upsample-1          [-1, 1, 224, 224]               0\n",
            "            Conv2d-2          [-1, 3, 224, 224]              30\n",
            "            Conv2d-3         [-1, 10, 224, 224]             280\n",
            "            Conv2d-4          [-1, 3, 224, 224]             273\n",
            "        Preprocess-5          [-1, 3, 224, 224]               0\n",
            "            Conv2d-6         [-1, 32, 112, 112]             864\n",
            "       BatchNorm2d-7         [-1, 32, 112, 112]              64\n",
            "             ReLU6-8         [-1, 32, 112, 112]               0\n",
            "            Conv2d-9         [-1, 32, 112, 112]             288\n",
            "      BatchNorm2d-10         [-1, 32, 112, 112]              64\n",
            "            ReLU6-11         [-1, 32, 112, 112]               0\n",
            "           Conv2d-12         [-1, 16, 112, 112]             512\n",
            "      BatchNorm2d-13         [-1, 16, 112, 112]              32\n",
            " InvertedResidual-14         [-1, 16, 112, 112]               0\n",
            "           Conv2d-15         [-1, 96, 112, 112]           1,536\n",
            "      BatchNorm2d-16         [-1, 96, 112, 112]             192\n",
            "            ReLU6-17         [-1, 96, 112, 112]               0\n",
            "           Conv2d-18           [-1, 96, 56, 56]             864\n",
            "      BatchNorm2d-19           [-1, 96, 56, 56]             192\n",
            "            ReLU6-20           [-1, 96, 56, 56]               0\n",
            "           Conv2d-21           [-1, 24, 56, 56]           2,304\n",
            "      BatchNorm2d-22           [-1, 24, 56, 56]              48\n",
            " InvertedResidual-23           [-1, 24, 56, 56]               0\n",
            "           Conv2d-24          [-1, 144, 56, 56]           3,456\n",
            "      BatchNorm2d-25          [-1, 144, 56, 56]             288\n",
            "            ReLU6-26          [-1, 144, 56, 56]               0\n",
            "           Conv2d-27          [-1, 144, 56, 56]           1,296\n",
            "      BatchNorm2d-28          [-1, 144, 56, 56]             288\n",
            "            ReLU6-29          [-1, 144, 56, 56]               0\n",
            "           Conv2d-30           [-1, 24, 56, 56]           3,456\n",
            "      BatchNorm2d-31           [-1, 24, 56, 56]              48\n",
            " InvertedResidual-32           [-1, 24, 56, 56]               0\n",
            "           Conv2d-33          [-1, 144, 56, 56]           3,456\n",
            "      BatchNorm2d-34          [-1, 144, 56, 56]             288\n",
            "            ReLU6-35          [-1, 144, 56, 56]               0\n",
            "           Conv2d-36          [-1, 144, 28, 28]           1,296\n",
            "      BatchNorm2d-37          [-1, 144, 28, 28]             288\n",
            "            ReLU6-38          [-1, 144, 28, 28]               0\n",
            "           Conv2d-39           [-1, 32, 28, 28]           4,608\n",
            "      BatchNorm2d-40           [-1, 32, 28, 28]              64\n",
            " InvertedResidual-41           [-1, 32, 28, 28]               0\n",
            "           Conv2d-42          [-1, 192, 28, 28]           6,144\n",
            "      BatchNorm2d-43          [-1, 192, 28, 28]             384\n",
            "            ReLU6-44          [-1, 192, 28, 28]               0\n",
            "           Conv2d-45          [-1, 192, 28, 28]           1,728\n",
            "      BatchNorm2d-46          [-1, 192, 28, 28]             384\n",
            "            ReLU6-47          [-1, 192, 28, 28]               0\n",
            "           Conv2d-48           [-1, 32, 28, 28]           6,144\n",
            "      BatchNorm2d-49           [-1, 32, 28, 28]              64\n",
            " InvertedResidual-50           [-1, 32, 28, 28]               0\n",
            "           Conv2d-51          [-1, 192, 28, 28]           6,144\n",
            "      BatchNorm2d-52          [-1, 192, 28, 28]             384\n",
            "            ReLU6-53          [-1, 192, 28, 28]               0\n",
            "           Conv2d-54          [-1, 192, 28, 28]           1,728\n",
            "      BatchNorm2d-55          [-1, 192, 28, 28]             384\n",
            "            ReLU6-56          [-1, 192, 28, 28]               0\n",
            "           Conv2d-57           [-1, 32, 28, 28]           6,144\n",
            "      BatchNorm2d-58           [-1, 32, 28, 28]              64\n",
            " InvertedResidual-59           [-1, 32, 28, 28]               0\n",
            "           Conv2d-60          [-1, 192, 28, 28]           6,144\n",
            "      BatchNorm2d-61          [-1, 192, 28, 28]             384\n",
            "            ReLU6-62          [-1, 192, 28, 28]               0\n",
            "           Conv2d-63          [-1, 192, 14, 14]           1,728\n",
            "      BatchNorm2d-64          [-1, 192, 14, 14]             384\n",
            "            ReLU6-65          [-1, 192, 14, 14]               0\n",
            "           Conv2d-66           [-1, 64, 14, 14]          12,288\n",
            "      BatchNorm2d-67           [-1, 64, 14, 14]             128\n",
            " InvertedResidual-68           [-1, 64, 14, 14]               0\n",
            "           Conv2d-69          [-1, 384, 14, 14]          24,576\n",
            "      BatchNorm2d-70          [-1, 384, 14, 14]             768\n",
            "            ReLU6-71          [-1, 384, 14, 14]               0\n",
            "           Conv2d-72          [-1, 384, 14, 14]           3,456\n",
            "      BatchNorm2d-73          [-1, 384, 14, 14]             768\n",
            "            ReLU6-74          [-1, 384, 14, 14]               0\n",
            "           Conv2d-75           [-1, 64, 14, 14]          24,576\n",
            "      BatchNorm2d-76           [-1, 64, 14, 14]             128\n",
            " InvertedResidual-77           [-1, 64, 14, 14]               0\n",
            "           Conv2d-78          [-1, 384, 14, 14]          24,576\n",
            "      BatchNorm2d-79          [-1, 384, 14, 14]             768\n",
            "            ReLU6-80          [-1, 384, 14, 14]               0\n",
            "           Conv2d-81          [-1, 384, 14, 14]           3,456\n",
            "      BatchNorm2d-82          [-1, 384, 14, 14]             768\n",
            "            ReLU6-83          [-1, 384, 14, 14]               0\n",
            "           Conv2d-84           [-1, 64, 14, 14]          24,576\n",
            "      BatchNorm2d-85           [-1, 64, 14, 14]             128\n",
            " InvertedResidual-86           [-1, 64, 14, 14]               0\n",
            "           Conv2d-87          [-1, 384, 14, 14]          24,576\n",
            "      BatchNorm2d-88          [-1, 384, 14, 14]             768\n",
            "            ReLU6-89          [-1, 384, 14, 14]               0\n",
            "           Conv2d-90          [-1, 384, 14, 14]           3,456\n",
            "      BatchNorm2d-91          [-1, 384, 14, 14]             768\n",
            "            ReLU6-92          [-1, 384, 14, 14]               0\n",
            "           Conv2d-93           [-1, 64, 14, 14]          24,576\n",
            "      BatchNorm2d-94           [-1, 64, 14, 14]             128\n",
            " InvertedResidual-95           [-1, 64, 14, 14]               0\n",
            "           Conv2d-96          [-1, 384, 14, 14]          24,576\n",
            "      BatchNorm2d-97          [-1, 384, 14, 14]             768\n",
            "            ReLU6-98          [-1, 384, 14, 14]               0\n",
            "           Conv2d-99          [-1, 384, 14, 14]           3,456\n",
            "     BatchNorm2d-100          [-1, 384, 14, 14]             768\n",
            "           ReLU6-101          [-1, 384, 14, 14]               0\n",
            "          Conv2d-102           [-1, 96, 14, 14]          36,864\n",
            "     BatchNorm2d-103           [-1, 96, 14, 14]             192\n",
            "InvertedResidual-104           [-1, 96, 14, 14]               0\n",
            "          Conv2d-105          [-1, 576, 14, 14]          55,296\n",
            "     BatchNorm2d-106          [-1, 576, 14, 14]           1,152\n",
            "           ReLU6-107          [-1, 576, 14, 14]               0\n",
            "          Conv2d-108          [-1, 576, 14, 14]           5,184\n",
            "     BatchNorm2d-109          [-1, 576, 14, 14]           1,152\n",
            "           ReLU6-110          [-1, 576, 14, 14]               0\n",
            "          Conv2d-111           [-1, 96, 14, 14]          55,296\n",
            "     BatchNorm2d-112           [-1, 96, 14, 14]             192\n",
            "InvertedResidual-113           [-1, 96, 14, 14]               0\n",
            "          Conv2d-114          [-1, 576, 14, 14]          55,296\n",
            "     BatchNorm2d-115          [-1, 576, 14, 14]           1,152\n",
            "           ReLU6-116          [-1, 576, 14, 14]               0\n",
            "          Conv2d-117          [-1, 576, 14, 14]           5,184\n",
            "     BatchNorm2d-118          [-1, 576, 14, 14]           1,152\n",
            "           ReLU6-119          [-1, 576, 14, 14]               0\n",
            "          Conv2d-120           [-1, 96, 14, 14]          55,296\n",
            "     BatchNorm2d-121           [-1, 96, 14, 14]             192\n",
            "InvertedResidual-122           [-1, 96, 14, 14]               0\n",
            "          Conv2d-123          [-1, 576, 14, 14]          55,296\n",
            "     BatchNorm2d-124          [-1, 576, 14, 14]           1,152\n",
            "           ReLU6-125          [-1, 576, 14, 14]               0\n",
            "          Conv2d-126            [-1, 576, 7, 7]           5,184\n",
            "     BatchNorm2d-127            [-1, 576, 7, 7]           1,152\n",
            "           ReLU6-128            [-1, 576, 7, 7]               0\n",
            "          Conv2d-129            [-1, 160, 7, 7]          92,160\n",
            "     BatchNorm2d-130            [-1, 160, 7, 7]             320\n",
            "InvertedResidual-131            [-1, 160, 7, 7]               0\n",
            "          Conv2d-132            [-1, 960, 7, 7]         153,600\n",
            "     BatchNorm2d-133            [-1, 960, 7, 7]           1,920\n",
            "           ReLU6-134            [-1, 960, 7, 7]               0\n",
            "          Conv2d-135            [-1, 960, 7, 7]           8,640\n",
            "     BatchNorm2d-136            [-1, 960, 7, 7]           1,920\n",
            "           ReLU6-137            [-1, 960, 7, 7]               0\n",
            "          Conv2d-138            [-1, 160, 7, 7]         153,600\n",
            "     BatchNorm2d-139            [-1, 160, 7, 7]             320\n",
            "InvertedResidual-140            [-1, 160, 7, 7]               0\n",
            "          Conv2d-141            [-1, 960, 7, 7]         153,600\n",
            "     BatchNorm2d-142            [-1, 960, 7, 7]           1,920\n",
            "           ReLU6-143            [-1, 960, 7, 7]               0\n",
            "          Conv2d-144            [-1, 960, 7, 7]           8,640\n",
            "     BatchNorm2d-145            [-1, 960, 7, 7]           1,920\n",
            "           ReLU6-146            [-1, 960, 7, 7]               0\n",
            "          Conv2d-147            [-1, 160, 7, 7]         153,600\n",
            "     BatchNorm2d-148            [-1, 160, 7, 7]             320\n",
            "InvertedResidual-149            [-1, 160, 7, 7]               0\n",
            "          Conv2d-150            [-1, 960, 7, 7]         153,600\n",
            "     BatchNorm2d-151            [-1, 960, 7, 7]           1,920\n",
            "           ReLU6-152            [-1, 960, 7, 7]               0\n",
            "          Conv2d-153            [-1, 960, 7, 7]           8,640\n",
            "     BatchNorm2d-154            [-1, 960, 7, 7]           1,920\n",
            "           ReLU6-155            [-1, 960, 7, 7]               0\n",
            "          Conv2d-156            [-1, 320, 7, 7]         307,200\n",
            "     BatchNorm2d-157            [-1, 320, 7, 7]             640\n",
            "InvertedResidual-158            [-1, 320, 7, 7]               0\n",
            "          Conv2d-159           [-1, 1280, 7, 7]         409,600\n",
            "     BatchNorm2d-160           [-1, 1280, 7, 7]           2,560\n",
            "           ReLU6-161           [-1, 1280, 7, 7]               0\n",
            "          Linear-162                  [-1, 100]         128,100\n",
            "     MobileNetV2-163                  [-1, 100]               0\n",
            "================================================================\n",
            "Total params: 2,352,555\n",
            "Trainable params: 2,352,555\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 160.51\n",
            "Params size (MB): 8.97\n",
            "Estimated Total Size (MB): 169.49\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq6Nn65yseRg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0005, amsgrad=True)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=10,gamma=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe_13PhNsmcH",
        "colab_type": "code",
        "outputId": "943e2d9e-2638-4ae6-daec-7eb043d1f457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "start_time = time.time()\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "train_acc_list = []\n",
        "val_acc_list = []\n",
        "for epoch in range(num_epochs):\n",
        "    train_accu = []\n",
        "    model.train()\n",
        "    train_running_loss = 0\n",
        "    for batch_idx, (X_train_batch, Y_train_batch) in enumerate(train_loader):\n",
        "        \n",
        "        X_train_batch,Y_train_batch = Variable(X_train_batch).cuda(),Variable(Y_train_batch).cuda()\n",
        "        \n",
        "        \n",
        "        output = model(X_train_batch)\n",
        "        loss = criterion(output, Y_train_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # if(epoch>6):\n",
        "        #     for group in optimizer.param_groups:\n",
        "        #         for p in group['params']:\n",
        "        #             state = optimizer.state[p]\n",
        "        #             if(state['step']>=1024):\n",
        "        #                 state['step'] = 1000\n",
        "        optimizer.step()\n",
        "        prediction = output.data.max(1)[1]\n",
        "        accuracy = ( float( prediction.eq(Y_train_batch.data).sum() ) /float(batch_size))*100.0\n",
        "        train_accu.append(accuracy)\n",
        "        train_running_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    train_acc = np.mean(train_accu)\n",
        "    train_loss.append(train_running_loss/len(train_loader))\n",
        "    train_acc_list.append(train_acc)\n",
        "# val the model\n",
        "    if(epoch==50):\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = learning_rate/10.0\n",
        "    if(epoch==75):\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = learning_rate/100.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        val_accu = []\n",
        "        val_running_loss = 0\n",
        "        for batch_idx, (X_val_batch, Y_val_batch) in enumerate(val_loader):\n",
        "            \n",
        "            X_val_batch, Y_val_batch= Variable(X_val_batch).cuda(),Variable(Y_val_batch).cuda()\n",
        "            \n",
        "            \n",
        "            output = model(X_val_batch)\n",
        "            loss = criterion(output, Y_val_batch)\n",
        "            val_running_loss += loss.item()\n",
        "            prediction = output.data.max(1)[1] \n",
        "            accuracy = ( float( prediction.eq(Y_val_batch.data).sum() ) /float(batch_size))*100.0\n",
        "            val_accu.append(accuracy)\n",
        "        val_acc = np.mean(val_accu)\n",
        "        val_loss.append(val_running_loss/len(val_loader))\n",
        "        val_acc_list.append(val_acc)\n",
        "    print('Epoch',epoch,'\\nTrain accuracy',train_acc,'validation accuracy',val_acc,'Running time',time.time()-start_time)\n",
        "    print('Training Loss = ',  train_loss[epoch], 'Val Loss =', val_loss[epoch])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2494: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 \n",
            "Train accuracy 40.82680626598466 validation accuracy 47.923259493670884 Running time 167.8452959060669\n",
            "Training Loss =  2.2462433183284674 Val Loss = 1.8460326496558854\n",
            "Epoch 1 \n",
            "Train accuracy 55.904331841432224 validation accuracy 55.07318037974684 Running time 335.7920792102814\n",
            "Training Loss =  1.5534824242677225 Val Loss = 1.5425589431690265\n",
            "Epoch 2 \n",
            "Train accuracy 61.79667519181586 validation accuracy 58.57397151898734 Running time 503.5487470626831\n",
            "Training Loss =  1.3357481736966106 Val Loss = 1.4304957133305223\n",
            "Epoch 3 \n",
            "Train accuracy 65.1974104859335 validation accuracy 60.57159810126582 Running time 671.2009723186493\n",
            "Training Loss =  1.1939653505754593 Val Loss = 1.3694714488862436\n",
            "Epoch 4 \n",
            "Train accuracy 67.7769341432225 validation accuracy 59.889240506329116 Running time 839.0132076740265\n",
            "Training Loss =  1.093120678154099 Val Loss = 1.3870880920675737\n",
            "Epoch 5 \n",
            "Train accuracy 70.13467071611254 validation accuracy 60.245253164556964 Running time 1006.9557859897614\n",
            "Training Loss =  1.0148380432287445 Val Loss = 1.3874684157250803\n",
            "Epoch 6 \n",
            "Train accuracy 71.94693094629156 validation accuracy 62.37143987341772 Running time 1174.8489730358124\n",
            "Training Loss =  0.9427282560207045 Val Loss = 1.31780890724327\n",
            "Epoch 7 \n",
            "Train accuracy 73.84311061381074 validation accuracy 62.94501582278481 Running time 1342.6807606220245\n",
            "Training Loss =  0.881239827179238 Val Loss = 1.307106177263622\n",
            "Epoch 8 \n",
            "Train accuracy 74.764226342711 validation accuracy 63.34058544303797 Running time 1510.511959552765\n",
            "Training Loss =  0.8387019602234101 Val Loss = 1.2811399698257446\n",
            "Epoch 9 \n",
            "Train accuracy 76.52853260869566 validation accuracy 63.014240506329116 Running time 1678.3513400554657\n",
            "Training Loss =  0.7787419740501267 Val Loss = 1.306084749064868\n",
            "Epoch 10 \n",
            "Train accuracy 84.23513427109974 validation accuracy 68.57199367088607 Running time 1846.2054224014282\n",
            "Training Loss =  0.5385222245207832 Val Loss = 1.100304995911031\n",
            "Epoch 11 \n",
            "Train accuracy 87.12036445012788 validation accuracy 68.1368670886076 Running time 2014.0515642166138\n",
            "Training Loss =  0.43935261015087135 Val Loss = 1.123214619823649\n",
            "Epoch 12 \n",
            "Train accuracy 88.83671675191816 validation accuracy 67.82041139240506 Running time 2181.7119176387787\n",
            "Training Loss =  0.3872007078221997 Val Loss = 1.151064599616618\n",
            "Epoch 13 \n",
            "Train accuracy 89.82976342710998 validation accuracy 67.64240506329114 Running time 2349.458511352539\n",
            "Training Loss =  0.3545022029096208 Val Loss = 1.1815013274361816\n",
            "Epoch 14 \n",
            "Train accuracy 90.78884271099744 validation accuracy 67.81052215189874 Running time 2517.2227330207825\n",
            "Training Loss =  0.3272391055398585 Val Loss = 1.193891080874431\n",
            "Epoch 15 \n",
            "Train accuracy 91.68198529411765 validation accuracy 67.71162974683544 Running time 2685.0312123298645\n",
            "Training Loss =  0.29946886265979095 Val Loss = 1.1873206601867192\n",
            "Epoch 16 \n",
            "Train accuracy 92.03964194373401 validation accuracy 66.73259493670886 Running time 2853.1780745983124\n",
            "Training Loss =  0.27962612332133074 Val Loss = 1.2176394877554495\n",
            "Epoch 17 \n",
            "Train accuracy 92.70899936061382 validation accuracy 66.69303797468355 Running time 3021.0802507400513\n",
            "Training Loss =  0.261964459057964 Val Loss = 1.2498706971542746\n",
            "Epoch 18 \n",
            "Train accuracy 93.45028772378517 validation accuracy 67.16772151898734 Running time 3189.051566839218\n",
            "Training Loss =  0.2421506259523694 Val Loss = 1.2315139687514003\n",
            "Epoch 19 \n",
            "Train accuracy 93.90784846547315 validation accuracy 67.37539556962025 Running time 3357.126451730728\n",
            "Training Loss =  0.2261153168576148 Val Loss = 1.2538005038152766\n",
            "Epoch 20 \n",
            "Train accuracy 97.16272378516624 validation accuracy 69.40268987341773 Running time 3524.863635778427\n",
            "Training Loss =  0.12857429535530718 Val Loss = 1.1714421774767623\n",
            "Epoch 21 \n",
            "Train accuracy 98.33959398976982 validation accuracy 69.75870253164557 Running time 3692.6272077560425\n",
            "Training Loss =  0.09143875222986617 Val Loss = 1.183393574968169\n",
            "Epoch 22 \n",
            "Train accuracy 98.80514705882354 validation accuracy 69.58069620253164 Running time 3860.4871323108673\n",
            "Training Loss =  0.07377116848021517 Val Loss = 1.1804138142851335\n",
            "Epoch 23 \n",
            "Train accuracy 99.07688618925832 validation accuracy 69.37302215189874 Running time 4028.635817050934\n",
            "Training Loss =  0.06312081489302314 Val Loss = 1.1945740671097478\n",
            "Epoch 24 \n",
            "Train accuracy 99.24472506393862 validation accuracy 69.52136075949367 Running time 4196.784400224686\n",
            "Training Loss =  0.05676254290906365 Val Loss = 1.1976496011395998\n",
            "Epoch 25 \n",
            "Train accuracy 99.40856777493606 validation accuracy 69.85759493670886 Running time 4365.003032684326\n",
            "Training Loss =  0.05053770171521265 Val Loss = 1.1978972456123256\n",
            "Epoch 26 \n",
            "Train accuracy 99.51446611253196 validation accuracy 69.66969936708861 Running time 4533.414907693863\n",
            "Training Loss =  0.04604836445196968 Val Loss = 1.1953033676630334\n",
            "Epoch 27 \n",
            "Train accuracy 99.55043158567774 validation accuracy 69.99604430379746 Running time 4701.567316055298\n",
            "Training Loss =  0.04226305654934605 Val Loss = 1.202329474159434\n",
            "Epoch 28 \n",
            "Train accuracy 99.57640664961637 validation accuracy 69.74881329113924 Running time 4869.51429104805\n",
            "Training Loss =  0.04044316936751156 Val Loss = 1.2089329724070392\n",
            "Epoch 29 \n",
            "Train accuracy 99.6343510230179 validation accuracy 69.75870253164557 Running time 5037.603326559067\n",
            "Training Loss =  0.037363444214396156 Val Loss = 1.2089671032338203\n",
            "Epoch 30 \n",
            "Train accuracy 99.76822250639387 validation accuracy 70.1443829113924 Running time 5205.685341835022\n",
            "Training Loss =  0.030132775897603204 Val Loss = 1.1890562418140942\n",
            "Epoch 31 \n",
            "Train accuracy 99.81817455242967 validation accuracy 70.09493670886076 Running time 5373.76161646843\n",
            "Training Loss =  0.025778530851540055 Val Loss = 1.1947728342647794\n",
            "Epoch 32 \n",
            "Train accuracy 99.80618606138107 validation accuracy 70.18393987341773 Running time 5541.696006774902\n",
            "Training Loss =  0.025574209711626363 Val Loss = 1.1964500327653522\n",
            "Epoch 33 \n",
            "Train accuracy 99.81817455242967 validation accuracy 70.09493670886076 Running time 5709.956112146378\n",
            "Training Loss =  0.02379414476358982 Val Loss = 1.1924823122688486\n",
            "Epoch 34 \n",
            "Train accuracy 99.81218030690538 validation accuracy 70.15427215189874 Running time 5878.2756588459015\n",
            "Training Loss =  0.02347796259786162 Val Loss = 1.1961036098154285\n",
            "Epoch 35 \n",
            "Train accuracy 99.84015345268543 validation accuracy 70.03560126582279 Running time 6046.481353759766\n",
            "Training Loss =  0.02243663254848984 Val Loss = 1.1919712522361852\n",
            "Epoch 36 \n",
            "Train accuracy 99.84015345268543 validation accuracy 70.19382911392405 Running time 6214.5440537929535\n",
            "Training Loss =  0.022330945681618607 Val Loss = 1.202951833417144\n",
            "Epoch 37 \n",
            "Train accuracy 99.84414961636828 validation accuracy 70.16416139240506 Running time 6382.563634157181\n",
            "Training Loss =  0.02213023365725337 Val Loss = 1.2029503863069075\n",
            "Epoch 38 \n",
            "Train accuracy 99.84614769820972 validation accuracy 69.87737341772151 Running time 6550.715169906616\n",
            "Training Loss =  0.02137550190948617 Val Loss = 1.20208012831362\n",
            "Epoch 39 \n",
            "Train accuracy 99.83615728900256 validation accuracy 69.73892405063292 Running time 6718.778652191162\n",
            "Training Loss =  0.021750056520676065 Val Loss = 1.2040934690946266\n",
            "Epoch 40 \n",
            "Train accuracy 99.87412084398977 validation accuracy 69.98615506329114 Running time 6886.760780334473\n",
            "Training Loss =  0.01976287618393788 Val Loss = 1.201177738135374\n",
            "Epoch 41 \n",
            "Train accuracy 99.87412084398977 validation accuracy 69.79825949367088 Running time 7054.808883905411\n",
            "Training Loss =  0.01947070394292512 Val Loss = 1.2028831251059906\n",
            "Epoch 42 \n",
            "Train accuracy 99.87212276214834 validation accuracy 69.91693037974683 Running time 7222.778478860855\n",
            "Training Loss =  0.01917879727414197 Val Loss = 1.1978215514859067\n",
            "Epoch 43 \n",
            "Train accuracy 99.87412084398977 validation accuracy 70.11471518987342 Running time 7390.872017860413\n",
            "Training Loss =  0.018709906950935988 Val Loss = 1.201170261147656\n",
            "Epoch 44 \n",
            "Train accuracy 99.87611892583121 validation accuracy 70.05537974683544 Running time 7558.856447696686\n",
            "Training Loss =  0.018518575755379085 Val Loss = 1.1996953253504596\n",
            "Epoch 45 \n",
            "Train accuracy 99.87212276214834 validation accuracy 69.96637658227849 Running time 7727.025814533234\n",
            "Training Loss =  0.018574060810267773 Val Loss = 1.1962413818021365\n",
            "Epoch 46 \n",
            "Train accuracy 99.87212276214834 validation accuracy 70.13449367088607 Running time 7895.2155973911285\n",
            "Training Loss =  0.018316457491091755 Val Loss = 1.2041856457915487\n",
            "Epoch 47 \n",
            "Train accuracy 99.87811700767263 validation accuracy 70.01582278481013 Running time 8063.150657892227\n",
            "Training Loss =  0.018362392368905075 Val Loss = 1.204219881492325\n",
            "Epoch 48 \n",
            "Train accuracy 99.88011508951406 validation accuracy 69.92681962025317 Running time 8231.006535768509\n",
            "Training Loss =  0.01806333553893944 Val Loss = 1.2046621404116666\n",
            "Epoch 49 \n",
            "Train accuracy 99.87611892583121 validation accuracy 70.2136075949367 Running time 8399.07290315628\n",
            "Training Loss =  0.01816061515446819 Val Loss = 1.2024558903295783\n",
            "Epoch 50 \n",
            "Train accuracy 99.88610933503837 validation accuracy 70.19382911392405 Running time 8567.099505901337\n",
            "Training Loss =  0.017797248995365084 Val Loss = 1.2027675818793382\n",
            "Epoch 51 \n",
            "Train accuracy 99.87212276214834 validation accuracy 70.11471518987342 Running time 8734.99720954895\n",
            "Training Loss =  0.018827406189325824 Val Loss = 1.2041414704503892\n",
            "Epoch 52 \n",
            "Train accuracy 99.88411125319693 validation accuracy 69.80814873417721 Running time 8902.974465847015\n",
            "Training Loss =  0.018016813151404985 Val Loss = 1.2046059445489812\n",
            "Epoch 53 \n",
            "Train accuracy 99.8821131713555 validation accuracy 69.92681962025317 Running time 9071.140061378479\n",
            "Training Loss =  0.01766191006106946 Val Loss = 1.2056298844421967\n",
            "Epoch 54 \n",
            "Train accuracy 99.87412084398977 validation accuracy 69.78837025316456 Running time 9239.288102149963\n",
            "Training Loss =  0.017359200679242154 Val Loss = 1.204014397120174\n",
            "Epoch 55 \n",
            "Train accuracy 99.88411125319693 validation accuracy 70.13449367088607 Running time 9407.418766260147\n",
            "Training Loss =  0.01723009618023015 Val Loss = 1.204044166245038\n",
            "Epoch 56 \n",
            "Train accuracy 99.88011508951406 validation accuracy 69.92681962025317 Running time 9575.625804901123\n",
            "Training Loss =  0.017290684873300134 Val Loss = 1.204982599880122\n",
            "Epoch 57 \n",
            "Train accuracy 99.8821131713555 validation accuracy 69.91693037974683 Running time 9743.958561897278\n",
            "Training Loss =  0.017204471608943037 Val Loss = 1.2047620785387256\n",
            "Epoch 58 \n",
            "Train accuracy 99.8821131713555 validation accuracy 69.9070411392405 Running time 9912.11988902092\n",
            "Training Loss =  0.01736133550162739 Val Loss = 1.204430042942868\n",
            "Epoch 59 \n",
            "Train accuracy 99.88011508951406 validation accuracy 70.12460443037975 Running time 10080.469785690308\n",
            "Training Loss =  0.017348982636695324 Val Loss = 1.2029443104055864\n",
            "Epoch 60 \n",
            "Train accuracy 99.87611892583121 validation accuracy 70.04549050632912 Running time 10248.61716890335\n",
            "Training Loss =  0.01707648959420526 Val Loss = 1.2022501615029346\n",
            "Epoch 61 \n",
            "Train accuracy 99.88810741687979 validation accuracy 69.91693037974683 Running time 10416.891525268555\n",
            "Training Loss =  0.016866540421953288 Val Loss = 1.2048115481304218\n",
            "Epoch 62 \n",
            "Train accuracy 99.8821131713555 validation accuracy 69.80814873417721 Running time 10585.29956650734\n",
            "Training Loss =  0.01711854084259104 Val Loss = 1.2061055287530151\n",
            "Epoch 63 \n",
            "Train accuracy 99.88011508951406 validation accuracy 69.96637658227849 Running time 10753.389295578003\n",
            "Training Loss =  0.016747782137864233 Val Loss = 1.203485283670546\n",
            "Epoch 64 \n",
            "Train accuracy 99.88610933503837 validation accuracy 70.0059335443038 Running time 10921.48830318451\n",
            "Training Loss =  0.01644034484577606 Val Loss = 1.2047344302829308\n",
            "Epoch 65 \n",
            "Train accuracy 99.87811700767263 validation accuracy 69.99604430379746 Running time 11089.802659511566\n",
            "Training Loss =  0.01649340609912677 Val Loss = 1.203941077371187\n",
            "Epoch 66 \n",
            "Train accuracy 99.88011508951406 validation accuracy 70.03560126582279 Running time 11257.838262796402\n",
            "Training Loss =  0.016926095754270207 Val Loss = 1.2027618681328207\n",
            "Epoch 67 \n",
            "Train accuracy 99.87611892583121 validation accuracy 69.95648734177215 Running time 11425.985144376755\n",
            "Training Loss =  0.016825169334402475 Val Loss = 1.201974400991126\n",
            "Epoch 68 \n",
            "Train accuracy 99.89010549872123 validation accuracy 69.88726265822785 Running time 11593.889944314957\n",
            "Training Loss =  0.016607479366195173 Val Loss = 1.206705292568931\n",
            "Epoch 69 \n",
            "Train accuracy 99.88011508951406 validation accuracy 70.01582278481013 Running time 11761.971862792969\n",
            "Training Loss =  0.016837068073584906 Val Loss = 1.2034461136105694\n",
            "Epoch 70 \n",
            "Train accuracy 99.87412084398977 validation accuracy 70.1443829113924 Running time 11930.03856921196\n",
            "Training Loss =  0.016587077404188987 Val Loss = 1.201431090318704\n",
            "Epoch 71 \n",
            "Train accuracy 99.88810741687979 validation accuracy 69.96637658227849 Running time 12098.091963768005\n",
            "Training Loss =  0.01644224579186391 Val Loss = 1.2061057611356807\n",
            "Epoch 72 \n",
            "Train accuracy 99.88610933503837 validation accuracy 70.01582278481013 Running time 12266.031707286835\n",
            "Training Loss =  0.01645951551359023 Val Loss = 1.2062467457372932\n",
            "Epoch 73 \n",
            "Train accuracy 99.88411125319693 validation accuracy 70.06526898734177 Running time 12434.12046790123\n",
            "Training Loss =  0.01669315192753168 Val Loss = 1.2031422725206689\n",
            "Epoch 74 \n",
            "Train accuracy 99.8821131713555 validation accuracy 69.81803797468355 Running time 12601.912213802338\n",
            "Training Loss =  0.01639544739938148 Val Loss = 1.2070831445199024\n",
            "Epoch 75 \n",
            "Train accuracy 99.8821131713555 validation accuracy 69.99604430379746 Running time 12769.799702882767\n",
            "Training Loss =  0.016689260292541034 Val Loss = 1.203963175604615\n",
            "Epoch 76 \n",
            "Train accuracy 99.8821131713555 validation accuracy 70.0751582278481 Running time 12937.802547693253\n",
            "Training Loss =  0.016672825445528224 Val Loss = 1.2050024482268322\n",
            "Epoch 77 \n",
            "Train accuracy 99.88011508951406 validation accuracy 70.13449367088607 Running time 13105.788727998734\n",
            "Training Loss =  0.01652480749999318 Val Loss = 1.2102987562553793\n",
            "Epoch 78 \n",
            "Train accuracy 99.88411125319693 validation accuracy 69.91693037974683 Running time 13273.8905646801\n",
            "Training Loss =  0.01641954313439634 Val Loss = 1.2029666334767886\n",
            "Epoch 79 \n",
            "Train accuracy 99.8821131713555 validation accuracy 69.9367088607595 Running time 13441.846328258514\n",
            "Training Loss =  0.01669728829789802 Val Loss = 1.205908589725253\n",
            "Epoch 80 \n",
            "Train accuracy 99.8821131713555 validation accuracy 70.2136075949367 Running time 13609.924997091293\n",
            "Training Loss =  0.016272818279045316 Val Loss = 1.2033874981011017\n",
            "Epoch 81 \n",
            "Train accuracy 99.88810741687979 validation accuracy 69.9367088607595 Running time 13777.813348293304\n",
            "Training Loss =  0.016369506645271235 Val Loss = 1.205766248552105\n",
            "Epoch 82 \n",
            "Train accuracy 99.89010549872123 validation accuracy 70.03560126582279 Running time 13945.653049945831\n",
            "Training Loss =  0.016100095766012932 Val Loss = 1.204535156111174\n",
            "Epoch 83 \n",
            "Train accuracy 99.88011508951406 validation accuracy 69.86748417721519 Running time 14113.521849155426\n",
            "Training Loss =  0.016388826319933547 Val Loss = 1.2032579817349398\n",
            "Epoch 84 \n",
            "Train accuracy 99.8821131713555 validation accuracy 69.89715189873418 Running time 14281.124150753021\n",
            "Training Loss =  0.01629887467912396 Val Loss = 1.2073008644429943\n",
            "Epoch 85 \n",
            "Train accuracy 99.89210358056266 validation accuracy 69.97626582278481 Running time 14449.095855474472\n",
            "Training Loss =  0.016290895695633748 Val Loss = 1.204334163213078\n",
            "Epoch 86 \n",
            "Train accuracy 99.87412084398977 validation accuracy 69.98615506329114 Running time 14616.94349360466\n",
            "Training Loss =  0.016486144974789656 Val Loss = 1.2087293395513221\n",
            "Epoch 87 \n",
            "Train accuracy 99.88610933503837 validation accuracy 70.0751582278481 Running time 14784.995803117752\n",
            "Training Loss =  0.016174020669649323 Val Loss = 1.2023769179476966\n",
            "Epoch 88 \n",
            "Train accuracy 99.88011508951406 validation accuracy 69.9070411392405 Running time 14952.7119576931\n",
            "Training Loss =  0.016326311851858788 Val Loss = 1.204218812381165\n",
            "Epoch 89 \n",
            "Train accuracy 99.88810741687979 validation accuracy 69.92681962025317 Running time 15120.81509900093\n",
            "Training Loss =  0.016459788593566022 Val Loss = 1.2073979966248138\n",
            "Epoch 90 \n",
            "Train accuracy 99.88610933503837 validation accuracy 70.03560126582279 Running time 15288.88974571228\n",
            "Training Loss =  0.01632203396571719 Val Loss = 1.2061637187305885\n",
            "Epoch 91 \n",
            "Train accuracy 99.89210358056266 validation accuracy 70.01582278481013 Running time 15456.739451885223\n",
            "Training Loss =  0.016314688922308595 Val Loss = 1.2059318038481701\n",
            "Epoch 92 \n",
            "Train accuracy 99.88810741687979 validation accuracy 69.94659810126582 Running time 15624.527377605438\n",
            "Training Loss =  0.016234207112351646 Val Loss = 1.2045793442786494\n",
            "Epoch 93 \n",
            "Train accuracy 99.88011508951406 validation accuracy 69.96637658227849 Running time 15792.292541503906\n",
            "Training Loss =  0.01627696837629656 Val Loss = 1.203995276101028\n",
            "Epoch 94 \n",
            "Train accuracy 99.88610933503837 validation accuracy 70.04549050632912 Running time 15960.22401213646\n",
            "Training Loss =  0.016379743497675795 Val Loss = 1.2016414735890641\n",
            "Epoch 95 \n",
            "Train accuracy 99.88411125319693 validation accuracy 69.99604430379746 Running time 16128.531356334686\n",
            "Training Loss =  0.016290179580030845 Val Loss = 1.2039292733880538\n",
            "Epoch 96 \n",
            "Train accuracy 99.88011508951406 validation accuracy 69.9367088607595 Running time 16296.71815443039\n",
            "Training Loss =  0.016292086047360963 Val Loss = 1.2083590234382242\n",
            "Epoch 97 \n",
            "Train accuracy 99.88411125319693 validation accuracy 69.9367088607595 Running time 16464.897765398026\n",
            "Training Loss =  0.016146033721239975 Val Loss = 1.2084641803669025\n",
            "Epoch 98 \n",
            "Train accuracy 99.88411125319693 validation accuracy 69.9367088607595 Running time 16633.178656816483\n",
            "Training Loss =  0.016310367799933303 Val Loss = 1.2071320648434796\n",
            "Epoch 99 \n",
            "Train accuracy 99.88411125319693 validation accuracy 69.85759493670886 Running time 16801.223457813263\n",
            "Training Loss =  0.016373649977928843 Val Loss = 1.2059053725834135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpRJD88PpHKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  n   import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1bd1tkhIe3C",
        "colab_type": "code",
        "outputId": "628e49cf-4a7c-4d20-b714-43968aeaec02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.plot(np.arange(50), train_acc_list[:50], color = 'r',label = 'train')\n",
        "plt.plot(np.arange(50), val_acc_list[:50], color = 'b', label = 'val')\n",
        "plt.title('Gray(1) CIFAR 100 on MobileNetV2--Pre-train')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnHPOFfGk4JxzrognBeecc0X+P6UE483gbKbTAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZwNh1zBVxF5",
        "colab_type": "code",
        "outputId": "632ff1f4-e345-49e9-ee17-822b63a8d8a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.plot(np.arange(50), train_loss[:50], color = 'r',label = 'train')\n",
        "plt.plot(np.arange(50), val_loss[:50], color = 'b', label = 'val')\n",
        "plt.title('Gray(1) CIFAR 100 on MobileNetV2--Pre-train')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zRKN5EBVxP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbne32U4rCoi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "#MobileNet\n",
        "\n",
        "def conv_bn(inp, oup, stride):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.ReLU6(inplace=True)\n",
        "    )\n",
        "\n",
        "\n",
        "def conv_1x1_bn(inp, oup):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.ReLU6(inplace=True)\n",
        "    )\n",
        "\n",
        "\n",
        "def make_divisible(x, divisible_by=8):\n",
        "    import numpy as np\n",
        "    return int(np.ceil(x * 1. / divisible_by) * divisible_by)\n",
        "\n",
        "\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(self, inp, oup, stride, expand_ratio):\n",
        "        super(InvertedResidual, self).__init__()\n",
        "        self.stride = stride\n",
        "        assert stride in [1, 2]\n",
        "\n",
        "        hidden_dim = int(inp * expand_ratio)\n",
        "        self.use_res_connect = self.stride == 1 and inp == oup\n",
        "\n",
        "        if expand_ratio == 1:\n",
        "            self.conv = nn.Sequential(\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.ReLU6(inplace=True),\n",
        "                \n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                #nn.Dropout(0.2),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "        else:\n",
        "            self.conv = nn.Sequential(\n",
        "                # pw\n",
        "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.ReLU6(inplace=True),\n",
        "                \n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.ReLU6(inplace=True),\n",
        "                #nn.Dropout(0.2),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "                #nn.Dropout(0.2)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_res_connect:\n",
        "            return x + self.conv(x)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class MobileNetV2(nn.Module):\n",
        "    def __init__(self, n_class=100, input_size=32, width_mult=1.):\n",
        "        super(MobileNetV2, self).__init__()\n",
        "        block = InvertedResidual\n",
        "        input_channel = 32\n",
        "        last_channel = 1280\n",
        "        interverted_residual_setting = [\n",
        "            # t, c, n, s\n",
        "            [1, 16, 1, 1],\n",
        "            [6, 24, 2, 1],\n",
        "            [6, 32, 3, 2],\n",
        "            [6, 64, 4, 1],\n",
        "            [6, 96, 3, 2],\n",
        "            [6, 160, 3, 1],\n",
        "            [6, 320, 1, 2],\n",
        "        ]\n",
        "\n",
        "        # building first layer\n",
        "        assert input_size % 32 == 0\n",
        "        # input_channel = make_divisible(input_channel * width_mult)  # first channel is always 32!\n",
        "        self.last_channel = make_divisible(last_channel * width_mult) if width_mult > 1.0 else last_channel\n",
        "        self.features = [conv_bn(1, input_channel, 1)]\n",
        "        # building inverted residual blocks\n",
        "        for t, c, n, s in interverted_residual_setting:\n",
        "            output_channel = make_divisible(c * width_mult) if t > 1 else c\n",
        "            for i in range(n):\n",
        "                if i == 0:\n",
        "                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n",
        "                else:\n",
        "                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n",
        "                input_channel = output_channel\n",
        "        # building last several layers\n",
        "        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n",
        "        # make it nn.Sequential\n",
        "        self.features = nn.Sequential(*self.features)\n",
        "\n",
        "        # building classifier\n",
        "        self.classifier = nn.Linear(self.last_channel, n_class)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.mean(3).mean(2)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                n = m.weight.size(1)\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "\n",
        "def mobilenet_v2(pretrained=True):\n",
        "    model = MobileNetV2(width_mult=1)\n",
        "\n",
        "    if pretrained:\n",
        "        try:\n",
        "            from torch.hub import load_state_dict_from_url\n",
        "        except ImportError:\n",
        "            from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "        state_dict = load_state_dict_from_url(\n",
        "            'https://www.dropbox.com/s/47tyzpofuuyyv1b/mobilenetv2_1.0-f2a8633.pth.tar?dl=1', progress=True)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXYbtWFe5fsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchsummary import summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJcUMtbPvhrR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=100):\n",
        "        self.inplanes = 32\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1,\n",
        "                              bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        #self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 32, layers[0],  stride=1)\n",
        "        self.layer2 = self._make_layer(block, 64, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 128, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 256, layers[3], stride=2)\n",
        "        #self.avgpool = nn.AvgPool2d(7, stride=1)\n",
        "        self.fc = nn.Linear(256 * block.expansion, num_classes)\n",
        "        self.dropout = torch.nn.Dropout2d(p=dropout_prob)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        #x = self.maxpool(x)\n",
        "        #print(x.shape)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        #x = self.avgpool(x)\n",
        "        # print(x.shape)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        # print(x.shape)\n",
        "       \n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbea4rD35tjb",
        "colab_type": "code",
        "outputId": "e3ef15ee-f9f8-47e0-9e80-a7102d30469d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model =  ResNet(BasicBlock,[2,4,4,2], num_classes=100)\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=256, out_features=100, bias=True)\n",
              "  (dropout): Dropout2d(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BksHRnoW6YEV",
        "colab_type": "code",
        "outputId": "095ab3cd-db03-4512-e541-4649ee2f30a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "summary(model, (3,32,32))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             864\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "         Dropout2d-4           [-1, 32, 32, 32]               0\n",
            "            Conv2d-5           [-1, 32, 32, 32]           9,216\n",
            "       BatchNorm2d-6           [-1, 32, 32, 32]              64\n",
            "              ReLU-7           [-1, 32, 32, 32]               0\n",
            "            Conv2d-8           [-1, 32, 32, 32]           9,216\n",
            "       BatchNorm2d-9           [-1, 32, 32, 32]              64\n",
            "             ReLU-10           [-1, 32, 32, 32]               0\n",
            "       BasicBlock-11           [-1, 32, 32, 32]               0\n",
            "           Conv2d-12           [-1, 32, 32, 32]           9,216\n",
            "      BatchNorm2d-13           [-1, 32, 32, 32]              64\n",
            "             ReLU-14           [-1, 32, 32, 32]               0\n",
            "           Conv2d-15           [-1, 32, 32, 32]           9,216\n",
            "      BatchNorm2d-16           [-1, 32, 32, 32]              64\n",
            "             ReLU-17           [-1, 32, 32, 32]               0\n",
            "       BasicBlock-18           [-1, 32, 32, 32]               0\n",
            "           Conv2d-19           [-1, 64, 16, 16]          18,432\n",
            "      BatchNorm2d-20           [-1, 64, 16, 16]             128\n",
            "             ReLU-21           [-1, 64, 16, 16]               0\n",
            "           Conv2d-22           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-23           [-1, 64, 16, 16]             128\n",
            "           Conv2d-24           [-1, 64, 16, 16]           2,048\n",
            "      BatchNorm2d-25           [-1, 64, 16, 16]             128\n",
            "             ReLU-26           [-1, 64, 16, 16]               0\n",
            "       BasicBlock-27           [-1, 64, 16, 16]               0\n",
            "           Conv2d-28           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-29           [-1, 64, 16, 16]             128\n",
            "             ReLU-30           [-1, 64, 16, 16]               0\n",
            "           Conv2d-31           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-32           [-1, 64, 16, 16]             128\n",
            "             ReLU-33           [-1, 64, 16, 16]               0\n",
            "       BasicBlock-34           [-1, 64, 16, 16]               0\n",
            "           Conv2d-35           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-36           [-1, 64, 16, 16]             128\n",
            "             ReLU-37           [-1, 64, 16, 16]               0\n",
            "           Conv2d-38           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-39           [-1, 64, 16, 16]             128\n",
            "             ReLU-40           [-1, 64, 16, 16]               0\n",
            "       BasicBlock-41           [-1, 64, 16, 16]               0\n",
            "           Conv2d-42           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-43           [-1, 64, 16, 16]             128\n",
            "             ReLU-44           [-1, 64, 16, 16]               0\n",
            "           Conv2d-45           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-46           [-1, 64, 16, 16]             128\n",
            "             ReLU-47           [-1, 64, 16, 16]               0\n",
            "       BasicBlock-48           [-1, 64, 16, 16]               0\n",
            "           Conv2d-49            [-1, 128, 8, 8]          73,728\n",
            "      BatchNorm2d-50            [-1, 128, 8, 8]             256\n",
            "             ReLU-51            [-1, 128, 8, 8]               0\n",
            "           Conv2d-52            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-53            [-1, 128, 8, 8]             256\n",
            "           Conv2d-54            [-1, 128, 8, 8]           8,192\n",
            "      BatchNorm2d-55            [-1, 128, 8, 8]             256\n",
            "             ReLU-56            [-1, 128, 8, 8]               0\n",
            "       BasicBlock-57            [-1, 128, 8, 8]               0\n",
            "           Conv2d-58            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-59            [-1, 128, 8, 8]             256\n",
            "             ReLU-60            [-1, 128, 8, 8]               0\n",
            "           Conv2d-61            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-62            [-1, 128, 8, 8]             256\n",
            "             ReLU-63            [-1, 128, 8, 8]               0\n",
            "       BasicBlock-64            [-1, 128, 8, 8]               0\n",
            "           Conv2d-65            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-66            [-1, 128, 8, 8]             256\n",
            "             ReLU-67            [-1, 128, 8, 8]               0\n",
            "           Conv2d-68            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-69            [-1, 128, 8, 8]             256\n",
            "             ReLU-70            [-1, 128, 8, 8]               0\n",
            "       BasicBlock-71            [-1, 128, 8, 8]               0\n",
            "           Conv2d-72            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-73            [-1, 128, 8, 8]             256\n",
            "             ReLU-74            [-1, 128, 8, 8]               0\n",
            "           Conv2d-75            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-76            [-1, 128, 8, 8]             256\n",
            "             ReLU-77            [-1, 128, 8, 8]               0\n",
            "       BasicBlock-78            [-1, 128, 8, 8]               0\n",
            "           Conv2d-79            [-1, 256, 4, 4]         294,912\n",
            "      BatchNorm2d-80            [-1, 256, 4, 4]             512\n",
            "             ReLU-81            [-1, 256, 4, 4]               0\n",
            "           Conv2d-82            [-1, 256, 4, 4]         589,824\n",
            "      BatchNorm2d-83            [-1, 256, 4, 4]             512\n",
            "           Conv2d-84            [-1, 256, 4, 4]          32,768\n",
            "      BatchNorm2d-85            [-1, 256, 4, 4]             512\n",
            "             ReLU-86            [-1, 256, 4, 4]               0\n",
            "       BasicBlock-87            [-1, 256, 4, 4]               0\n",
            "           Conv2d-88            [-1, 256, 4, 4]         589,824\n",
            "      BatchNorm2d-89            [-1, 256, 4, 4]             512\n",
            "             ReLU-90            [-1, 256, 4, 4]               0\n",
            "           Conv2d-91            [-1, 256, 4, 4]         589,824\n",
            "      BatchNorm2d-92            [-1, 256, 4, 4]             512\n",
            "             ReLU-93            [-1, 256, 4, 4]               0\n",
            "       BasicBlock-94            [-1, 256, 4, 4]               0\n",
            "           Linear-95                  [-1, 100]          25,700\n",
            "================================================================\n",
            "Total params: 3,559,556\n",
            "Trainable params: 3,559,556\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 10.63\n",
            "Params size (MB): 13.58\n",
            "Estimated Total Size (MB): 24.22\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcH3XKeR6cUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}